# Large Language Menagerie

A Taxonomy of open-source and/or open-weights LLMs.

At the current pace of innovation, this table can become quickly outdated. Feel free to open PRs to improve it or keep it up-to-date.

| Model Name   | Institution | First release date | Source Code License | Weights License | Dataset License | Dataset Size | Dataset Language(s)  | Model Size | Base Model | Training modality | Comments |
|--------------|-------------|--------------------|---------------------|-----------------|-----------------|--------------|----------------------|------------|--------------|--------------------|----------|
| [Flan-T5-{}](https://huggingface.co/google/flan-t5-xl) | Google    | October 2022  | Apache 2.0 | Apache 2.0 | N/A | N/A | Many languages | 80M-11B | T5 |   Instruction fine-tuned  |    |
| [GPT4All](https://github.com/nomic-ai/gpt4all)          | Nomic-ai | March 2023      | Apache License 2.0 | | | 800k examples | English | 11B Params | GPT-J | Instruction & dialog fine-tuned  | |
| [LLaMA](https://github.com/facebookresearch/llama)          | Meta | February 2023      | GPL-3  | Non-commercial bespoke license | N/A | >1 tokens | 20 languages | 7B, 13B, 33B, and 65B  | N/A | Causal LM   | First highly performant "small" LLM |
| [Alpaca](https://github.com/tatsu-lab/stanford_alpaca)          | Stanford | March 2023   |Apache License 2.00 | CC BY NC 4.0 (LLaMA weigth diff) | Claims CC BY NC 4.0 but not clear it is!| 54k examples | English | 7B, 13B | LLaMA | Instruction fine-tuned  | Similar qualitative performance as OpenAI's text-davinci-003, fine-tuned at a cost of ~600USD (dataset gen + training). First evidence that small-high quality data can make a relatively small LLM competitive with much bigger models. |
| [VicuÃ±a](https://github.com/lm-sys/FastChat) |  UC Berkeley, UCSD, CMU, MBZUAI | March 2023   | Apache License 2.0 | Apache License 2.0 (LLaMA weigth diff) | N/A | 70k examples | N/A | 13B | LLaMA | Instruction & dialog fine-tuned  | Another LLaMA with model, which GPT-4 grades better than ChatGPT and Alpaca.  Fine-tuned at a cost of ~300USD |
| [Koala](https://bair.berkeley.edu/blog/2023/04/03/koala/) | BAIR (Berkeley) | April 2023 | Apache License 2.0 | Unclear, claimed to be "subject to the model License of LLaMA, Terms of Use of the data generated by OpenAI, and Privacy Practices of ShareGPT. Any other usage of the model weights, including but not limited to commercial usage, is strictly prohibited. " | N/A | >350k examples | N/A | 13B | LLaMA | Instruction & dialog fine-tuned  | Another LLaMA with model fine-tuned on large, partially propietary dialog dataset, with comparable performance to Alpaca according to human evaluators. |
