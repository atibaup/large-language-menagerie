# Large Language Menagerie

A Taxonomy of open-source and/or open-weights LLMs.

At the current pace of innovation, this table can become quickly outdated. Feel free to open PRs to improve it or keep it up-to-date.

| Model Name   | Institution | First release date | Source Code License | Weights License | Dataset License | Dataset Size | Dataset Language(s)  | Model Size | Base Model | Training modality | <div style="width:290px"> Comments </div>|
|--------------|-------------|--------------------|---------------------|-----------------|-----------------|--------------|----------------------|------------|--------------|--------------------|----------|
| [Flan-T5-{}](https://huggingface.co/google/flan-t5-xl) | Google    | October 2022  | Apache 2.0 | Apache 2.0 | N/A | N/A | Many languages | 80M-11B | T5 |   Instruction fine-tuned  |    |
| [GPT4All](https://github.com/nomic-ai/gpt4all)          | Nomic-ai | March 2023      | Apache License 2.0 | | | 800k examples | English | 11B Params | GPT-J | Instruction & dialog fine-tuned  | |
| [LLaMA](https://github.com/facebookresearch/llama)          | Meta | February 2023      | GPL-3  | Non-commercial bespoke license | N/A | >1 tokens | 20 languages | 7B, 13B, 33B, and 65B  | N/A | Causal LM   | First highly performant "small" LLM |
| [Alpaca](https://github.com/tatsu-lab/stanford_alpaca)          | Stanford | March 2023   |Apache License 2.00 | CC BY NC 4.0 (LLaMA weigth diff) | Claims CC BY NC 4.0 but not clear it is!| 54k examples | English | 7B, 13B | LLaMA | Instruction fine-tuned  | [Alpaca](#alpaca)|
| [VicuÃ±a](https://github.com/lm-sys/FastChat) |  UC Berkeley, UCSD, CMU, MBZUAI | March 2023   | Apache License 2.0 | Apache License 2.0 (LLaMA weigth diff) | N/A | 70k examples | N/A | 13B | LLaMA | Instruction & dialog fine-tuned  | [Vicuna](#vicuna) |
| [Koala](https://bair.berkeley.edu/blog/2023/04/03/koala/) | BAIR (Berkeley) | April 2023 | Apache License 2.0 | Unclear [1](#1) | N/A | >350k examples | N/A | 13B | LLaMA | Instruction & dialog fine-tuned  | [Koala](#koala) |
| [FastChat-T5](https://huggingface.co/lmsys/fastchat-t5-3b-v1.0) | UC Berkeley, UCSD, CMU, MBZUAI  | April 2023 | Apache License 2.0 | Unclear [2](#2) | N/A | 70k examples | N/A | 3B | T5-flan-XL | Dialog fine-tuned  |  |
| [Pythia](https://github.com/EleutherAI/pythia) | EleutherAI  | April 2023 | Apache License 2.0 | Apache License 2.0 | "open source" [3](#3) | 300B tokens | Mostly English (though multiple languages) | 70M-12B | GPT-neoX | Causal LM |  |
| [StableLM-Alpha](https://github.com/Stability-AI/StableLM) | Stability-AI  | April 2023 | N/A | CC BY-SA-4.0 | N/A | 1.5T tokens | Mostly English (though multiple languages) | 3B-7B | GPT-neoX | Causal LM | 4k context window, training code not available |


<a name="1"></a> [1] Claimed to be "subject to the model License of LLaMA, Terms of Use of the data generated by OpenAI, and Privacy Practices of ShareGPT. Any other usage of the model weights, including but not limited to commercial usage, is strictly prohibited. "

<a name="2"></a> [2] Uses ShareGPT data, which comes form users posting data from ChatGPT, whose terms and conditions are restrictive...

<a name="3"></a> [3] The code to generate the pile is MIT-licensed, and the data itself can be downloaded, no-strings-attached from [here](https://pile.eleuther.ai/). But nowhere it says what the actual license for the dataset is, other than claiming it is "open-source".

## Comments

- <a name="alpaca"></a>Alpaca: First evidence that small-high quality data can make a relatively small LLM competitive with much bigger models, LLaMA fine-tuned at a cost of ~600USD (dataset gen + training)
- <a name="vicuna"></a>Alpaca Another LLaMA with model, which GPT-4 grades better than ChatGPT and Alpaca.  Fine-tuned at a cost of ~300USD 
- <a name="koala"></a> Another LLaMA with model fine-tuned on large, partially propietary dialog dataset, with comparable performance to Alpaca according to human evaluators.
